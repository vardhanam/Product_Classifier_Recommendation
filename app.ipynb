{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff3cea4-1d8b-469d-9a67-4b8ae2fceaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ebf612-df0d-4c97-a9cd-a103f4956a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'path_to_csv' with the actual path to your CSV file\n",
    "csv_file_path = 'styles.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path, on_bad_lines='skip')\n",
    "\n",
    "# Now 'df' holds the DataFrame object with the data from the CSV file\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77687bac-e6c6-43e5-ac8f-13b01beac02c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Item_List = list(set(df['articleType'].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a98db17-2918-4d84-9e68-2a26377c8bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "model = CLIPModel.from_pretrained(model_id)\n",
    "\n",
    "# move model to device if possible\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ddcb4-8ef2-4a6c-9b79-b8f4bbddfe82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = processor(\n",
    "    text=Item_List,\n",
    "    padding=True,\n",
    "    images=None,\n",
    "    return_tensors='pt'\n",
    ").to(device)\n",
    "tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131b87aa-a505-47a0-8002-077584a021da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_emb = model.get_text_features(\n",
    "    **tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab789af0-93c4-44a6-8d90-75a668ec349e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_emb_list = text_emb.detach().cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107571df-e536-403c-9fc0-69e23aa2480c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"text_embeddings\",\n",
    "    vectors_config=models.VectorParams(size=512, distance=models.Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b2d048-d8a8-4f54-bf95-03e6dc76dca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert list into a list of dictionaries\n",
    "payload = [{'itemtype': item} for item in Item_List]\n",
    "\n",
    "# Show the result\n",
    "print(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c8e48a-b8e2-41d2-8d4d-ac01de4af921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a list of IDs from 1 to the length of the items list\n",
    "id_list = [i for i in range(1, len(Item_List) + 1)]\n",
    "\n",
    "# Show the result\n",
    "print(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21827f16-285b-4c2e-bcf4-33289076b6f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.upsert(\n",
    "    collection_name=\"text_embeddings\",\n",
    "    points=models.Batch(\n",
    "        ids=id_list,\n",
    "        payloads=payload,\n",
    "        vectors=text_emb_list,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ca2fa1c5-559f-483e-84b3-23651246253b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image_query_vector = processor(\n",
    "\t\ttext = None,\n",
    "\t\timages = Image.open('images/1578.jpg'),\n",
    "\t\treturn_tensors=\"pt\"\n",
    "\t\t)[\"pixel_values\"].to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a3e76cf7-fece-4a56-99b7-1fbe50d56f52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_query_vector_emb = model.get_image_features(image_query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0901f5d-91e7-4184-9d9f-5c0031c33791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_query_vector_emb.detach().cpu().numpy().tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e1f436bd-d7dd-44d3-9c33-1cc09681cf6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/6388.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "95cace23-b948-44c9-aec6-04e835f5b56d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id=25, version=0, score=0.28064984, payload={'itemtype': 'Innerwear Vests'}, vector=None, shard_key=None),\n",
       " ScoredPoint(id=83, version=0, score=0.24891837, payload={'itemtype': 'Lounge Tshirts'}, vector=None, shard_key=None),\n",
       " ScoredPoint(id=70, version=0, score=0.24561983, payload={'itemtype': 'Camisoles'}, vector=None, shard_key=None)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.search(\n",
    "    collection_name=\"text_embeddings\",\n",
    "    query_vector=image_query_vector_emb.detach().cpu().numpy().tolist()[0],\n",
    "    limit=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc95a43-363b-436c-86bc-3201012b4abc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.delete_collection(collection_name=\"image_embeddings\")\n",
    "client.create_collection(\n",
    "    collection_name=\"image_embeddings\",\n",
    "    vectors_config=models.VectorParams(size=512, distance=models.Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd32ad-0f64-4fa6-bb2e-70563b17be5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory where the images are stored\n",
    "image_directory = 'images'\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop over all files in the image directory\n",
    "for i, filename in enumerate(os.listdir(image_directory)):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):  # Add any other file types if needed\n",
    "        # Construct the full path to the image file\n",
    "        file_path = os.path.join(image_directory, filename)\n",
    "        with Image.open(file_path) as img:\n",
    "             \n",
    "            # Prepare the image for the model\n",
    "            tokens = processor(\n",
    "                text=None,\n",
    "                images=img,\n",
    "                return_tensors=\"pt\"\n",
    "            )[\"pixel_values\"].to(device)\n",
    "        \n",
    "            # Get image embeddings from the model\n",
    "            image_embeddings = model.get_image_features(\n",
    "                tokens\n",
    "            )\n",
    "        \n",
    "            # Append the filename and embeddings to the data list\n",
    "            data.append((filename, image_embeddings.detach().cpu().numpy().tolist()[0])\n",
    "\n",
    "    # Print the iteration number\n",
    "    print(f'Iteration {i}: Processed {filename}')\n",
    "\n",
    "# Create a DataFrame with the data\n",
    "df = pd.DataFrame(data, columns=['Filename', 'ImageEmbeddings'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfef561-cd05-468d-a137-3888cdd50afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming 'df' is your existing DataFrame\n",
    "# Create a new 'ID' column that starts at 1 and increments by 1 for each row\n",
    "df['ID'] = range(1, len(df) + 1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fddc30-1956-4d82-a161-c778fde6fcea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the list of dictionaries with \"Filename\" as the key\n",
    "filenames_list = [{'Filename': filename} for filename in df['Filename']]\n",
    "\n",
    "# Create the list of ImageEmbeddings (assuming ImageTensor is already a list of embeddings)\n",
    "image_embeddings_list = df['ImageEmbeddings'].tolist()\n",
    "\n",
    "# Create the list of IDs\n",
    "ids_list = df['ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccdc44e-a059-4e27-baf4-116dfa0ce130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming 'client' is your database client and 'models' is a module providing the Batch class\n",
    "# And assuming your lists are named 'ids_list', 'filenames_list', and 'image_embeddings_list'\n",
    "\n",
    "batch_size = 1000\n",
    "total_points = len(ids_list)\n",
    "\n",
    "for start_index in range(0, total_points, batch_size):\n",
    "    # End index is the start of the next batch or the end of the list\n",
    "    end_index = min(start_index + batch_size, total_points)\n",
    "    \n",
    "    # Slice the lists to create the current batch\n",
    "    batch_ids = ids_list[start_index:end_index]\n",
    "    batch_filenames = filenames_list[start_index:end_index]\n",
    "    batch_image_embeddings = image_embeddings_list[start_index:end_index]\n",
    "    \n",
    "    # Upsert the current batch\n",
    "    client.upsert(\n",
    "        collection_name=\"image_embeddings\",\n",
    "        points=models.Batch(\n",
    "            ids=batch_ids,\n",
    "            payloads=batch_filenames,\n",
    "            vectors=batch_image_embeddings,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c05a1e-2970-473c-b860-6122dbd4fe85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.scroll(collection_name=\"image_embeddings\", limit=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ceb6795e-45e2-4abc-a1c3-7c3884f293bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_classifier(image):\n",
    "   # Prepare the image for the model\n",
    "    tokens = processor(\n",
    "        text=None,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\"\n",
    "    )[\"pixel_values\"].to(device)\n",
    "\n",
    "    # Get image embeddings from the model\n",
    "    image_embeddings = model.get_image_features(\n",
    "        tokens\n",
    "    )\n",
    "\n",
    "    query_vector = image_embeddings.detach().cpu().numpy().tolist()[0]\n",
    "        \n",
    "    record = client.search(\n",
    "        collection_name=\"text_embeddings\",\n",
    "        query_vector=query_vector,\n",
    "        limit=1,\n",
    "    )\n",
    "    \n",
    "    return record[0].payload['itemtype']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a629b106-ecbb-4d0e-950b-b449dbae6a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_path_list(image):\n",
    "       # Prepare the image for the model\n",
    "    tokens = processor(\n",
    "        text=None,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\"\n",
    "    )[\"pixel_values\"].to(device)\n",
    "\n",
    "    # Get image embeddings from the model\n",
    "    image_embeddings = model.get_image_features(\n",
    "        tokens\n",
    "    )\n",
    "\n",
    "    query_vector = image_embeddings.detach().cpu().numpy().tolist()[0]\n",
    "        \n",
    "    record = client.search(\n",
    "        collection_name=\"image_embeddings\",\n",
    "        query_vector=query_vector,\n",
    "        limit=10,\n",
    "    )\n",
    "    \n",
    "    return [('fashion-dataset/fashion-dataset/images/' + element.payload['Filename'], None) for element in record]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "51e9ec40-bb0d-4036-933a-b6fb96ef3fff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7867\n",
      "Running on public URL: https://9fd542f880fe689b07.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9fd542f880fe689b07.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    with gr.Row():\n",
    "        upload_image = gr.Image(label= \"Upload Your Image\", type = 'pil')\n",
    "        classifier_text = gr.Textbox(label= \"Type of Item\")\n",
    "        \n",
    "    with gr.Row():\n",
    "        image_gallery = gr.Gallery(label= \"Similar items in the inventory\", object_fit= 'contain', columns=[5], rows=[2],)\n",
    "        \n",
    "    with gr.Row():\n",
    "        clr_btn = gr.Button(value= \"Clear\")\n",
    "    \n",
    "    first_step = upload_image.upload(fn= image_classifier, inputs= upload_image, outputs= classifier_text)\n",
    "    first_step.then(fn= image_path_list, inputs= upload_image, outputs = image_gallery)\n",
    "    clr_btn.click(fn=lambda: (None, None, []), inputs=None, outputs=[upload_image, classifier_text, image_gallery])\n",
    "    \n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
